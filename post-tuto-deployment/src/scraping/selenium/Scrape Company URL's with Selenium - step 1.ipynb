{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll fetch the urls of the companies that are reference on trustpilot.com\n",
    "\n",
    "We'll use selenium because the content is dynamically rendered\n",
    "\n",
    "We'll then scrape the reviews using scrapy and feed it the scraped urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "base_url = \"https://trustpilot.com\"\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    return BeautifulSoup(requests.get(url).content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by fetching sub-categories urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "soup = get_soup(base_url + '/categories')\n",
    "for category in soup.findAll('div', {'class': 'category-object'}):\n",
    "    name = category.find('h3', {'class': 'sub-category__header'}).text\n",
    "    name = name.strip()\n",
    "    data[name] = {}  \n",
    "    sub_categories = category.find('div', {'class': 'sub-category-list'})\n",
    "    for sub_category in sub_categories.findAll('div', {'class': 'child-category'}):\n",
    "        sub_category_name = sub_category.find('a', {'class': 'sub-category-item'}).text \n",
    "        sub_category_uri = sub_category.find('a', {'class': 'sub-category-item'})['href'] \n",
    "        data[name][sub_category_name] = sub_category_uri\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to fetch company urls reference in a given subcategory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_urls_form_page():\n",
    "    a_list = driver.find_elements_by_xpath('//a[@class=\"category-business-card card\"]')\n",
    "    urls = [a.get_attribute('href') for a in a_list]\n",
    "    dedup_urls = list(set(urls))\n",
    "    return dedup_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions indicates if a pagination exists on the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_next_page():\n",
    "    try:\n",
    "        button = driver.find_element_by_xpath('//a[@class=\"button button--primary next-page\"]')\n",
    "        return True, button\n",
    "    except NoSuchElementException:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing Selenium with a headless Chromedriver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('start-maximized')\n",
    "options.add_argument('disable-infobars')\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\isaama2\\Documents\\Chromedriver\\chromedriver.exe', options=options)\n",
    "\n",
    "timeout = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timeout variable is the time (in seconds) Selenium waits for a page to completely load.\n",
    "\n",
    "Now we launch the scraping. This approcimately takes 50 minutes with good internet conntection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_urls = {}\n",
    "for category in tqdm.notebook.tqdm(data):\n",
    "    for sub_category in tqdm.notebook.tqdm(data[category], leave=False):\n",
    "        company_urls[sub_category] = []\n",
    "\n",
    "        url = base_url + data[category][sub_category] + \"?numberofreviews=0&timeperiod=0&status=all\"\n",
    "        driver.get(url)\n",
    "        try: \n",
    "            element_present = EC.presence_of_element_located(\n",
    "                (By.CLASS_NAME, 'category-business-card card'))\n",
    "\n",
    "            WebDriverWait(driver, timeout).until(element_present)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        next_page = True\n",
    "        c = 1\n",
    "        while next_page:\n",
    "            extracted_company_urls = extract_company_urls_form_page()\n",
    "            company_urls[sub_category] += extracted_company_urls\n",
    "            next_page, button = go_next_page()\n",
    "\n",
    "            if next_page:\n",
    "                c += 1\n",
    "                next_url = base_url + data[category][sub_category] + \"?numberofreviews=0&timeperiod=0&status=all\" + f'&page={c}'\n",
    "                driver.get(next_url)\n",
    "                try: \n",
    "                    element_present = EC.presence_of_element_located(\n",
    "                        (By.CLASS_NAME, 'category-business-card card'))\n",
    "\n",
    "                    WebDriverWait(driver, timeout).until(element_present)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the scraping is over, we save the company urls to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "consolidated_data = []\n",
    "\n",
    "for category in data:\n",
    "    for sub_category in data[category]:\n",
    "        for url in company_urls[sub_category]:\n",
    "            consolidated_data.append((category, sub_category, url))\n",
    "            \n",
    "df_consolidated_data = pd.DataFrame(consolidated_data, columns=['category', 'sub_category', 'company_url'])\n",
    "\n",
    "os.chdir(r'C:\\Users\\isaama2\\Documents\\GitHub\\Trustpilot')\n",
    "df_consolidated_data.to_csv('company_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
